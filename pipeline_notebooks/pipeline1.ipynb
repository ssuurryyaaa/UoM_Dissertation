{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2q-0TlEdZ9i",
        "outputId": "739527fe-f40a-4355-f1ea-9f74c9f0f5cc"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import csv\n",
        "import warnings\n",
        "from pandas import isnull\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "pattern = r'(_d[1-8][ab])'\n",
        "mapped_seq_df = pd.DataFrame()\n",
        "list_of_sequences = []\n",
        "\n",
        "# PLEASE CHANGE THE PATH ACCORDINGLY\n",
        "your_path = '/Users/surya/Documents/UoM_Dissertation/'\n",
        "\n",
        "# /Users/surya/Documents/UoM_Dissertation/artifacts/data/raw/Interaction Data\n",
        "\n",
        "raw_path = your_path + 'artifacts/data/raw/'\n",
        "preprocess_path =  your_path + 'artifacts/data/preprocessing/'\n",
        "prediction_path = your_path + 'artifacts/data/prediction/'\n",
        "json_path = raw_path + 'Interaction Data/'\n",
        "json_files = os.listdir(json_path)\n",
        "\n",
        "unwanted_columns = [ 'screen', 'client', 'episodeCount',\n",
        "       'sessionstartms', 'timezoneOffset', 'sd', 'sid', 'url', 'urlFull',\n",
        "       'scroll', 'mouse', 'delta', 'key', 'change', 'select'\n",
        "       'link', 'class', 'img', 'type', 'inheritedId', 'name', 'textValue', 'text', 'textContent']\n",
        "\n",
        "necessary_columns = ['event', 'timestampms','node', 'user','id', 'dom', 'flag', 'task', 'merged_event']\n",
        "\n",
        "total_columns = ['_id', 'event', 'screen', 'client', 'episodeCount', 'timestampms',\n",
        "       'sessionstartms', 'timezoneOffset', 'sd', 'sid', 'url', 'urlFull',\n",
        "       'mouse', 'node', 'scroll', 'delta', 'key', 'select', 'change', 'user',\n",
        "       'type', 'textContent', 'text', 'link', 'inheritedId', 'img', 'class',\n",
        "       'id', 'textValue', 'name', 'dom', 'flag', 'task', 'merged_event']\n",
        "\n",
        "id_values = ['toDashboard1', 'toDashboard2', 'toDashboard3', 'toDashboard4', 'toDashboard5',\n",
        "             'toDashboard6', 'toDashboard7', 'toDashboard8', 'toDashboard9', 'toDashboard10',\n",
        "             'toDashboard11', 'toDashboard12', 'toDashboard13', 'toDashboard14', 'toDashboard15',\n",
        "             'toDashboard16']\n",
        "\n",
        "id_value_until_practice_tutorial = id_values[0]\n",
        "\n",
        "combined_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ym1ps0_6lbb"
      },
      "source": [
        "### function definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjDcmfa3Mgs9"
      },
      "outputs": [],
      "source": [
        "### function definitions\n",
        "\n",
        "def reading_json_files(json_data):\n",
        "  json_data = re.sub(r'ObjectId\\((.+?)\\)', r'\\1', json_data)\n",
        "  tempdf = pd.read_json(json_data, lines=True)\n",
        "  return tempdf\n",
        "\n",
        "\n",
        "def cleaning_one(tempdf):\n",
        "  tempdf['user'] = i\n",
        "  tempdf.drop(['mobileTouch','evID'], axis=1, inplace = True, errors='ignore')\n",
        "  return tempdf\n",
        "\n",
        "def splitting_json_column_values(df):\n",
        "      # Split the key-value pairs in the node column\n",
        "      df['node'] = df['node'].apply(lambda x: {} if pd.isnull(x) else x)  # Convert NaN values to empty dictionaries\n",
        "      # Extract the keys from the node dictionary\n",
        "      keys = set()\n",
        "      for node_dict in df['node']:\n",
        "          keys.update(node_dict.keys())\n",
        "      # Create new columns for each key\n",
        "      for key in keys:\n",
        "          df[key] = [node_dict.get(key) if isinstance(node_dict, dict) else None for node_dict in df['node']]\n",
        "\n",
        "def updating_dom_element(df):\n",
        "  count = df[df['dom'].notna() & df['dom'].str.contains('\\)')].shape[0]\n",
        "  # Strip the substring before ')' for non-NA values\n",
        "  df.loc[df['dom'].notna() & df['dom'].str.contains('\\)'), 'dom'] = df['dom'].str.split('\\)').str[-1]\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def removing_rows_of_practice_tutorial(df):\n",
        "  # filling NA values\n",
        "  df['id'].fillna(-555, inplace=True)\n",
        "  # ignoring the indices until the practise tutorial\n",
        "  last_index = df[df['id'] == id_value_until_practice_tutorial].index[-1]\n",
        "  # Step 2: Remove all rows before the last index\n",
        "  df = df.loc[last_index:]\n",
        "  # Reset the index if desired\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "def removing_mid_values_between_toDashboardN_ids(df):\n",
        "  # Remove mid rows between 'toDashboardN' first and last occurrences\n",
        "  first_occurrences = {}\n",
        "  last_occurrences = {}\n",
        "  for value in range(1, 17):\n",
        "      id_value = f'toDashboard{value}'\n",
        "      first_occurrences[id_value] = df[df['id'] == id_value].index[0]\n",
        "      last_occurrences[id_value] = df[df['id'] == id_value].index[-1]\n",
        "  # Step 2: Remove the rows between the first and last occurrence for each value\n",
        "  for value, first_index in first_occurrences.items():\n",
        "      last_index = last_occurrences[value]\n",
        "      df = df.drop(index=range(first_index , last_index))\n",
        "\n",
        "def dropping_mid_rows_between_toDashboardN_ids(df):\n",
        "  dropped_rows_count = {id_value: 0 for id_value in id_values}\n",
        "  for id_value in id_values:\n",
        "      # Get the indices of the first and last occurrence of the id value\n",
        "      first_occurrence = df.index[df['id'] == id_value].min()\n",
        "      last_occurrence = df.index[df['id'] == id_value].max()\n",
        "      # Determine the indices of the middle rows between the first and last occurrence\n",
        "      middle_rows_indices = list(range(first_occurrence , last_occurrence))\n",
        "      dropped_rows_count[id_value] = len(middle_rows_indices)\n",
        "      df.drop(middle_rows_indices, inplace=True)\n",
        "\n",
        "  # Reset the index of the dataframe\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def flagging_toDashboard_rows(df):\n",
        "  df['flag'] = False\n",
        "  # Step 2: Set the flag to True for rows matching the 'toDashboard' values\n",
        "  for value in range(1, 17):\n",
        "      id_value = f'toDashboard{value}'\n",
        "      df.loc[df['id'] == id_value, 'flag'] = True\n",
        "      df.reset_index(drop= True, inplace=True)\n",
        "\n",
        "def merging_events(df):\n",
        "    df['merged_event'] = df['event']\n",
        "    # Replace 'mousewheel' with 'scroll'\n",
        "    df['merged_event'] = df['merged_event'].replace('mousewheel', 'scroll')\n",
        "    # Merge rows based on the initial conditions\n",
        "    # df.loc[(df['merged_event'] == 'mouseover') & (df['merged_event'].shift(1) == 'mouseout'), 'merged_event'] = 'hover'\n",
        "    df['merged_event'] = df['merged_event'].str.replace('mouseover', 'hover').str.replace('mouseout', 'hover')\n",
        "\n",
        "    df['merged_event'] = df['merged_event'].str.replace('mouseup', 'click').str.replace('mousedown', 'click')\n",
        "    # df.loc[(df['merged_event'] == 'mouseup') & (df['merged_event'].shift(1) == 'mousedown'), 'merged_event'] = 'click'\n",
        "    df['merged_event'] = df['merged_event'].str.replace('keyup', 'press').str.replace('keydown', 'press')\n",
        "    df['merged_event'] = df['merged_event'].str.replace('mobileTouchStart', 'click').str.replace('mobileTouchEnd', 'click')\n",
        "\n",
        "    # df.loc[(df['merged_event'] == 'keyup') & (df['merged_event'].shift(1) == 'keydown'), 'merged_event'] = 'press'\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def merging_multiple_events(df):\n",
        "    df['multi_merged_event'] = df['merged_event']\n",
        "    # Merge consecutive rows based on the additional conditions\n",
        "    df.loc[(df['merged_event'].shift(1) == 'hover') & (df['merged_event'] == 'hover'), 'multi_merged_event'] = 'multihover'\n",
        "    df.loc[(df['merged_event'].shift(1) == 'click') & (df['merged_event'] == 'click'), 'multi_merged_event'] = 'multiclick'\n",
        "    df.loc[(df['merged_event'].shift(1) == 'press') & (df['merged_event'] == 'press'), 'multi_merged_event'] = 'multipress'\n",
        "    df.loc[(df['merged_event'].shift(1) == 'scroll') & (df['merged_event'] == 'scroll'), 'multi_merged_event'] = 'multiscroll'\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def merging_events_master(df):\n",
        "    df['event_master'] = df['multi_merged_event']\n",
        "    # Merge consecutive rows based on the additional conditions\n",
        "    df.loc[(df['multi_merged_event'].shift(1) == 'mouseout') & (df['multi_merged_event'] == 'multihover'), 'event_master'] = 'multihover'\n",
        "    df.loc[(df['multi_merged_event'].shift(1) == 'mouseout') & (df['multi_merged_event'] == 'hover'), 'event_master'] = 'multihover'\n",
        "    df.loc[(df['multi_merged_event'].shift(1) == 'hover') & (df['multi_merged_event'] == 'mouseout'), 'event_master'] = 'multihover'\n",
        "    df.loc[(df['multi_merged_event'].shift(1) == 'multihover') & (df['multi_merged_event'] == 'mouseout'), 'event_master'] = 'multihover'\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def dropping_multi_hoverings(df):\n",
        "  df['m_event'] = df['event_master'].ne(df['event_master'].shift()).cumsum()\n",
        "  df = df[df['event_master'] != 'multihover']  # Remove 'mhover' rows\n",
        "  df = df.drop_duplicates(subset='m_event', keep='first').drop(columns='m_event')\n",
        "\n",
        "\n",
        "def initialising_new_column_task(df):\n",
        "  df['task'] = df['id'].str.extract(pattern)\n",
        "  df.loc[df['flag'], 'task'] = True\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def getting_unique_task_values(df_name):\n",
        "      temp_df = df_name  # Access the DataFrame dynamically using globals()\n",
        "      # Step 1: Find unique non-NaN values in the 'task' column\n",
        "      unique_values = temp_df['task'].unique()\n",
        "      unique_task = next((x for x in unique_values if isinstance(x, str)), None)\n",
        "      # Step 2: Check if any unique value is a string\n",
        "      string_pattern = None\n",
        "      return unique_task\n",
        "\n",
        "def ignoring_na_dom_values(df):\n",
        "  df = df[(df['dom'].notna()) | (df['flag'] == True)]\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def getting_dataframes_task_wise_index(df):\n",
        "  indices = df.index[df['flag']]\n",
        "  for i in range(len(indices)-1):\n",
        "      start = indices[i]\n",
        "      end = indices[i+1]\n",
        "      df_name = f\"df_db{i+1}\"\n",
        "      globals()[df_name] = df.loc[start:end-1].copy()\n",
        "  # Create the last DataFrame df_db16 from the last True value to the last row\n",
        "  start = indices[-1]\n",
        "  end = df.index[-1]\n",
        "  df_db16 = df.loc[start:end-1].copy()\n",
        "  task_wise_split_list = []\n",
        "  # Step 3: Print the separate DataFrames\n",
        "  for i in range(1, 16):\n",
        "      df_name = f\"df_db{i}\"\n",
        "      task_wise_split_list.append(globals()[df_name])\n",
        "  task_wise_split_list.append(df_db16)\n",
        "  return task_wise_split_list\n",
        "\n",
        "def dropping_na_values_from_itemset_columns(df_name):\n",
        "  df_name.loc[df_name['dom'] == '', 'dom'] = np.nan\n",
        "  df_name.dropna(subset=['dom'], inplace=True)\n",
        "\n",
        "def dropping_consecutive_rows(df, old_col, new_col):\n",
        "  consecutive_duplicates = (df['final_event'] == df['final_event'].shift(1))\n",
        "  # Drop duplicates, keeping only the first occurrence of each consecutive series\n",
        "  df = df[~consecutive_duplicates]\n",
        "\n",
        "def dropping_single_transformed_events(df):\n",
        "  rows_to_remove = df[df['merged_event'] != df['event']].index -1\n",
        "  # Step 1: Filter rows to remove to avoid KeyError\n",
        "  valid_rows_to_remove = [idx for idx in rows_to_remove if idx in df.index]\n",
        "  # Step 2: Remove the rows from the DataFrame\n",
        "  df.drop(valid_rows_to_remove, inplace=True)\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "def dropping_multi_transformed_events(df):\n",
        "  rows_to_remove = df[df['multi_merged_event'] != df['merged_event']].index -1\n",
        "  # Step 1: Filter rows to remove to avoid KeyError\n",
        "  valid_rows_to_remove = [idx for idx in rows_to_remove if idx in df.index]\n",
        "  # Step 2: Remove the rows from the DataFrame\n",
        "  df.drop(valid_rows_to_remove, inplace=True)\n",
        "\n",
        "def dropping_events_master(df):\n",
        "  rows_to_remove = df[df['event_master'] != df['multi_merged_event']].index -1\n",
        "  # Step 1: Filter rows to remove to avoid KeyError\n",
        "  valid_rows_to_remove = [idx for idx in rows_to_remove if idx in df.index]\n",
        "  # Step 2: Remove the rows from the DataFrame\n",
        "  df.drop(valid_rows_to_remove, inplace=True)\n",
        "\n",
        "def defining_sequence(series_list):\n",
        "  # Concatenate all the series into a single series\n",
        "  concatenated_series = pd.concat(series_list, ignore_index=True)\n",
        "  # Append -2 to the end of the final series\n",
        "  concatenated_series = concatenated_series.append(pd.Series([-2]))\n",
        "  # removing rows with substring of nan values or index True values from 'Task' column\n",
        "  sequence = concatenated_series[~concatenated_series.apply(lambda x: str(x).endswith(('Tru_e')))]\n",
        "  return sequence\n",
        "\n",
        "def replacing_unique_strings_with_numbers(string_1):\n",
        "    # Split the big string into words and remove duplicates\n",
        "    words = string_1.split()\n",
        "    unique_strings = list(set(words))\n",
        "    # Remove '-1' and '-2' from the list of unique strings\n",
        "    unique_strings = [s for s in unique_strings if s not in ['-1', '-2']]\n",
        "    # Create a mapping dictionary between original string values and numbers\n",
        "    num_mapping = {}\n",
        "    next_number = 1  # Start assigning numbers from 1000\n",
        "    for unique_string in unique_strings:\n",
        "        num_mapping[unique_string] = str(next_number)\n",
        "        next_number += 1\n",
        "    # Replace the unique string values in the big string with their corresponding numbers\n",
        "    for i in range(len(words)):\n",
        "        if words[i] in num_mapping:\n",
        "            words[i] = num_mapping[words[i]]\n",
        "    # Join the words back into a new string\n",
        "    new_string = ' '.join(words)\n",
        "    # Create a DataFrame to store the mapping between the original and new string values\n",
        "    mapping_df = pd.DataFrame(list(num_mapping.items()), columns=['Original_String', 'New_Number'])\n",
        "    return new_string, mapping_df\n",
        "\n",
        "def splitting_string_on_negative_two(input_string):\n",
        "    # Split the input_string whenever \"-2\" is encountered\n",
        "    output_string = re.sub(r'-2\\s', '-2\\n', input_string)\n",
        "    return output_string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3D6ujWMOoSV"
      },
      "outputs": [],
      "source": [
        "def defining_itemset(df_name, val):\n",
        "  df_name['task_val'] =  val\n",
        "  dropping_na_values_from_itemset_columns(df_name)\n",
        "  dropping_single_transformed_events(df_name)\n",
        "  merging_multiple_events(df_name)\n",
        "  dropping_multi_transformed_events(df_name)\n",
        "  merging_events_master(df_name)\n",
        "  dropping_events_master(df_name)\n",
        "\n",
        "  df_name['itemset'] = df_name['event_master'].astype(str) + \"_\" + df_name['type'].astype(str)\n",
        "  # df_name['itemset'] = df_name['merged_event'].astype(str) + \"_\" + df_name['id'].astype(str) + df_name['dom'].astype(str)\n",
        "  return df_name['itemset']\n",
        "\n",
        "def cleaning_three(df_task_list):\n",
        "  itemset_column = []\n",
        "  user_sequence = []\n",
        "  temp_list = []\n",
        "  for inner_df in df_task_list:\n",
        "    it = defining_itemset(inner_df, getting_unique_task_values(inner_df))\n",
        "    itemset_column.append(it.append(pd.Series([-1])))\n",
        "    temp_list.append(inner_df)\n",
        "  concatenated_series = pd.concat(itemset_column, ignore_index=True)\n",
        "  # Step 2: Convert the concatenated Series to a single line string\n",
        "  single_line = \" \".join(concatenated_series.astype(str))\n",
        "  single_line = single_line + \" -2\\n\"\n",
        "  # user_sequence.append(defining_sequence(itemset_column))\n",
        "  return single_line, temp_list\n",
        "\n",
        "def cleaning_two(data_list):\n",
        "  df_task_list = []\n",
        "  one_concatened_dt_list = []\n",
        "  master_line = ''\n",
        "  for df in data_list:\n",
        "      splitting_json_column_values(df)\n",
        "      updating_dom_element(df)\n",
        "      removing_rows_of_practice_tutorial(df)\n",
        "      dropping_mid_rows_between_toDashboardN_ids(df)\n",
        "      flagging_toDashboard_rows(df)\n",
        "      initialising_new_column_task(df)\n",
        "      getting_unique_task_values(df)\n",
        "      merging_events(df)\n",
        "      ignoring_na_dom_values(df)\n",
        "      df_task_list = getting_dataframes_task_wise_index(df)\n",
        "      line, temp_list = cleaning_three(df_task_list)\n",
        "      temp_df = pd.concat(temp_list, ignore_index=True)\n",
        "      one_concatened_dt_list.append(temp_df)\n",
        "      master_line = master_line + line\n",
        "  combined_df = pd.concat(one_concatened_dt_list, ignore_index=True)\n",
        "  return master_line, combined_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ_WvepqOSKN"
      },
      "outputs": [],
      "source": [
        "def combining_similar_types(temp_dataframe):\n",
        "  temp_dataframe['updated_type'] = temp_dataframe['type']\n",
        "  type_text = ['B', 'STRONG', 'I', 'P', 'GRAMMARLY-MIRROR', 'GRAMMARLY-EXTENSION', 'CENTER' , 'A']\n",
        "  type_list = ['LI', 'UL']\n",
        "  type_heading = ['H2', 'H3', 'H4', 'H5', 'HR']\n",
        "  type_table = ['TD', 'TH']\n",
        "  type_button = ['COM-1PASSWORD-BUTTON']\n",
        "  type_graphic = ['rect', 'tspan', 'path', 'IMG']\n",
        "  type_basic = ['HTML', 'BODY']\n",
        "  type_select = ['SELECT', 'OPTION']\n",
        "\n",
        "  # Use .loc to update 'updated_type' column where 'type' column values match the values_to_update\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_text), 'updated_type'] = 'TEXT'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_heading), 'updated_type'] = 'HEADING'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_table), 'updated_type'] = 'TABLE'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_button), 'updated_type'] = 'BUTTON'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_graphic), 'updated_type'] = 'GRAPHIC'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_basic), 'updated_type'] = 'BASIC'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_select), 'updated_type'] = 'SELECTION'\n",
        "  temp_dataframe.loc[temp_dataframe['type'].isin(type_list), 'updated_type'] = 'LISTING'\n",
        "  return temp_dataframe\n",
        "\n",
        "def finding_consecutive_pairs(df):\n",
        "    # Create a new column 'consecutive_pair' to store consecutive pairs\n",
        "    df['consecutive_master_pair'] = df['event_master'] + ' -> ' + df['event_master'].shift(-1)\n",
        "    df['consecutive_itemset_pair'] = df['itemset'] + ' -> ' + df['itemset'].shift(-1)\n",
        "    # Drop the last row as it will have a NaN value for the 'shift(-1)' operation\n",
        "    df.drop(df.index[-1], inplace=True)\n",
        "    # Print the consecutive pairs of values in 'event' column\n",
        "    merged_consecutive_pairs = df['consecutive_master_pair'].unique()\n",
        "\n",
        "def removing_consecutive_duplicates(df, column_name, event):\n",
        "    mask = (df[column_name] != event) | (df[column_name].shift() != event)\n",
        "    df_two = df[mask].copy()\n",
        "    while True:\n",
        "        mask = (df_two[column_name] != event) | (df_two[column_name].shift() != event)\n",
        "        temp_df = df_two[mask]\n",
        "        if temp_df.equals(df_two):\n",
        "            break\n",
        "        df_two = temp_df.copy()\n",
        "    return df_two\n",
        "\n",
        "def replacing_consecutive_events(df, column_name, old_event, new_event):\n",
        "    mask = (df[column_name] == old_event) & (df[column_name].shift() == old_event)\n",
        "    df.loc[mask, column_name] = new_event\n",
        "    return df\n",
        "\n",
        "def merging_click_sequences_recursive(df, column_name, event, new_event):\n",
        "    mask = (df[column_name] != event) | (df[column_name].shift() != event)\n",
        "    df = df[mask]\n",
        "    if any(df[column_name] == event):\n",
        "        df = replacing_consecutive_events(df, column_name, event, new_event)\n",
        "        return merging_click_sequences_recursive(df, column_name, event, new_event)\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "def common_values_across_db(series_list):\n",
        "  # Initialize a set with the values from the first series\n",
        "  common_values = set(series_list[0])\n",
        "  # Iterate through the remaining series and update the common_values set\n",
        "  for series in series_list[1:]:\n",
        "      common_values.intersection_update(series)\n",
        "  # Convert the common_values set back to a list if needed\n",
        "  common_values_list = list(common_values)\n",
        "  # Print the common values\n",
        "  print(\"Common values:\", common_values_list)\n",
        "\n",
        "def user_mapping_with_timestamp(cleaned_df):\n",
        "  user_mapping = cleaned_df[['user','sessionstartms']]\n",
        "  list_time = user_mapping.sessionstartms.unique()\n",
        "  data = { 'User': np.arange(1, len(list_time) + 1), 'Timestamp': list_time }\n",
        "  user_time = pd.DataFrame(data)\n",
        "  user_time.to_csv(preprocess_path+'usertimee.csv',index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmi_pn6Z3U0B"
      },
      "outputs": [],
      "source": [
        "data_list = [None] * len(json_files)\n",
        "i = 1\n",
        "for file in json_files:\n",
        "    with open(json_path + file, 'r') as fi:\n",
        "        json_data = fi.read()\n",
        "    tempdf = cleaning_one(reading_json_files(json_data))\n",
        "    data_list[i-1] = tempdf\n",
        "    i = i+1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYjNMgOB10Ux",
        "outputId": "907fb29d-1797-4b61-8ab0-0b96b476c872"
      },
      "outputs": [],
      "source": [
        "master_df = pd.concat(data_list, ignore_index=True)\n",
        "master_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6BLwTH_pii6",
        "outputId": "16b0d684-7ba2-4d42-943d-9d2bf81c677a"
      },
      "outputs": [],
      "source": [
        "master_df.url.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WiAOTgupO41",
        "outputId": "7edee2e9-48bf-4a8a-d8e9-1e239d535c86"
      },
      "outputs": [],
      "source": [
        "master_df.timezoneOffset.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "0XQt2F62L4In",
        "outputId": "5324f62a-95c0-4adf-eab8-180a233964b8"
      },
      "outputs": [],
      "source": [
        "master_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc8UCLhofEyY",
        "outputId": "8ed1dbb5-ed6a-40aa-9b2e-101bfdeb6665"
      },
      "outputs": [],
      "source": [
        "master_df.event.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "RFZQxPnU3A6m",
        "outputId": "c166ba1a-846a-4ca6-b06b-4abcd3dc56d1"
      },
      "outputs": [],
      "source": [
        "file_value, cleaned_df = cleaning_two(data_list)\n",
        "user_mapping_with_timestamp(cleaned_df)\n",
        "cleaned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_oNZdQGVECN",
        "outputId": "11cacfc1-bd05-41b9-bd88-950412fbe43e"
      },
      "outputs": [],
      "source": [
        "cleaned_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSK8xmfoS5Mq",
        "outputId": "b4625b38-543a-41df-a0d9-aad1bab4fcc6"
      },
      "outputs": [],
      "source": [
        "cleaned_df.type.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiDbvYLbQmvJ"
      },
      "outputs": [],
      "source": [
        "final_string, mapping_dataframe = replacing_unique_strings_with_numbers(file_value)\n",
        "output_string = splitting_string_on_negative_two(final_string)\n",
        "# Create and write the output to a text file named 'output.txt'\n",
        "with open(preprocess_path +  'one.txt', 'w') as file:\n",
        "    file.write(output_string)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVikP0wB-ZdR"
      },
      "outputs": [],
      "source": [
        "finding_consecutive_pairs(cleaned_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jQojHv3GbOa",
        "outputId": "0fc4de23-8149-4581-8502-0578076c9401"
      },
      "outputs": [],
      "source": [
        "temp_dataframe = cleaned_df[['event_master','type', 'itemset', 'id','class','dom','user','task_val', 'consecutive_master_pair', 'consecutive_itemset_pair']]\n",
        "df_two = removing_consecutive_duplicates(combining_similar_types(temp_dataframe), 'event_master', 'multihover')\n",
        "df_two.dropna(subset=['task_val'], inplace=True)\n",
        "finding_consecutive_pairs(df_two)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "Vj8AmfjQagAL",
        "outputId": "898beba5-020f-487c-a58b-fe42de48b41e"
      },
      "outputs": [],
      "source": [
        "df_two"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3htK_UQ8rgK"
      },
      "outputs": [],
      "source": [
        "sorted_df = df_two.sort_values(by=['user','task_val'])\n",
        "unique_task_val = sorted_df['task_val'].unique()\n",
        "unique_user_val = sorted_df['user'].unique()\n",
        "\n",
        "# Create an empty list to store the DataFrames\n",
        "dataframe_sorted_db_list = []\n",
        "dataframe_sorted_user_db_list = []\n",
        "\n",
        "db_user_list = []\n",
        "for user in unique_user_val:\n",
        "    user_task_df = sorted_df[sorted_df['user'] == user].copy()\n",
        "    dataframe_sorted_user_db_list.append(user_task_df)\n",
        "\n",
        "mapped_dashboard_unique_itemsets = []\n",
        "user_dashboard_wise_seq = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-cTqhbGSsrH"
      },
      "outputs": [],
      "source": [
        "full_seq = ''\n",
        "for df in dataframe_sorted_user_db_list:\n",
        "  itemset_column = []\n",
        "  concatenated_series = pd.Series()\n",
        "  unique_its = []\n",
        "  for task in unique_task_val:\n",
        "    task_df = df[df['task_val'] == task].copy()\n",
        "    task_df['its'] = task_df['event_master'].astype(str) + '__' + task_df['updated_type'].astype(str)\n",
        "    unique_its.append(task_df['its'])\n",
        "    itemset_column.append(task_df['its'].append(pd.Series([-1])))\n",
        "  concatenated_series = pd.concat(itemset_column, ignore_index=True)\n",
        "  single_line = \" \".join(concatenated_series.astype(str))\n",
        "  single_line = single_line + \" -2\\n\"\n",
        "  full_seq = full_seq + single_line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "yyZ-YJ6Uogz6",
        "outputId": "09d6cd5d-88b3-4cea-cf12-8121f9b74e02"
      },
      "outputs": [],
      "source": [
        "final, mapping_user_task_dataframe = replacing_unique_strings_with_numbers(full_seq)\n",
        "output_full_string = splitting_string_on_negative_two(final)\n",
        "# Create and write the output to a text file named 'output.txt'\n",
        "with open(preprocess_path + 'all_string.txt', 'w') as file:\n",
        "    file.write(output_full_string)\n",
        "mapping_user_task_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWv2f6msraeh"
      },
      "outputs": [],
      "source": [
        "mapping_user_task_dataframe.to_csv(preprocess_path + \"Mapped_values.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpMNj-sAh_H-"
      },
      "outputs": [],
      "source": [
        "# sorting logic\n",
        "\n",
        "sorted_df = df_two.sort_values(by=['task_val','user'])\n",
        "unique_task_val = sorted_df['task_val'].unique()\n",
        "unique_user_val = sorted_df['user'].unique()\n",
        "\n",
        "dataframe_sorted_db_list = []\n",
        "dashboard_wise_seq = []\n",
        "\n",
        "for task in unique_task_val:\n",
        "    user_task_df = sorted_df[sorted_df['task_val'] == task].copy()\n",
        "    dataframe_sorted_db_list.append(user_task_df)\n",
        "\n",
        "for df in dataframe_sorted_db_list:\n",
        "  itemset_column = []\n",
        "  concatenated_series = pd.Series()\n",
        "  for user_no in unique_user_val:\n",
        "    user_df = df[df['user'] == user_no].copy()\n",
        "    user_df['its'] = user_df['event_master'].astype(str) + '__' + user_df['updated_type'].astype(str) + '__' + user_df['user'].astype(str)\n",
        "    itemset_column.append(user_df['its'].append(pd.Series([-1])))\n",
        "\n",
        "  concatenated_series = pd.concat(itemset_column, ignore_index=True)\n",
        "  single_line = \" \".join(concatenated_series.astype(str))\n",
        "  single_line = single_line + \" -2\\n\"\n",
        "\n",
        "  str1, map1 = replacing_unique_strings_with_numbers(single_line)\n",
        "  mapped_dashboard_unique_itemsets.append(map1)\n",
        "  dashboard_wise_seq.append(str1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "JTFHF3tdtvqe",
        "outputId": "3aa6410c-5390-47dd-aaf2-69b0f0891f77"
      },
      "outputs": [],
      "source": [
        "dataframe_sorted_db_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zuOnKZf4o57B",
        "outputId": "6aa49c69-5090-4979-d264-969772fc0814"
      },
      "outputs": [],
      "source": [
        "mapped_dashboard_unique_itemsets[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "mkNltj1kraO3",
        "outputId": "283357ff-12ad-44d3-9c39-06559c41e9d8"
      },
      "outputs": [],
      "source": [
        "dashboard_wise_seq[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZTRIp2kWxuc"
      },
      "outputs": [],
      "source": [
        "merged_string = ''.join(dashboard_wise_seq)\n",
        "op_merged = splitting_string_on_negative_two(merged_string)\n",
        "with open(preprocess_path + 'op_merged.txt', 'w') as fi:\n",
        "      fi.write(op_merged)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht-N-pP2DOGC",
        "outputId": "01be8ff6-9036-4356-b2c1-3b753ce415ad"
      },
      "outputs": [],
      "source": [
        "mapped_dashboard_unique_itemsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaMACmk9F4o2"
      },
      "outputs": [],
      "source": [
        "df_two.to_csv(preprocess_path + \"two.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0qEOorIEI6d"
      },
      "outputs": [],
      "source": [
        "finding_consecutive_pairs(df_two)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi1jsTloCTcs",
        "outputId": "f35853a7-a573-405f-f071-3a50e5873493"
      },
      "outputs": [],
      "source": [
        "df_two.consecutive_master_pair.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2gNkZijERP_",
        "outputId": "29ba5b2d-378f-48a9-fc75-50d9ac0e5f80"
      },
      "outputs": [],
      "source": [
        "df_two.consecutive_itemset_pair.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdczc0s9Ea68",
        "outputId": "fb9168bc-fa8c-4df2-e0f5-f13f2b2b1fe2"
      },
      "outputs": [],
      "source": [
        "cleaned_df.event_master.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL0nWC4cuMej",
        "outputId": "acf1e7a6-b969-44d0-d8db-01789faf7ca3"
      },
      "outputs": [],
      "source": [
        "cleaned_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HjcnWOcISPS"
      },
      "outputs": [],
      "source": [
        "user_mapping = cleaned_df[['user','sessionstartms']]\n",
        "list_time = user_mapping.sessionstartms.unique()\n",
        "data = { 'User': np.arange(1, len(list_time) + 1), 'Timestamp': list_time }\n",
        "user_time = pd.DataFrame(data)\n",
        "user_time.to_csv(preprocess_path + 'usertime.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.9 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "fffc98e9d899e0ff87a477507a6a9f98998a0bb2d09fd35614b9d54e3c15b729"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
